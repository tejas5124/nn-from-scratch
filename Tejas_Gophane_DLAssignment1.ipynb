{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Neural Network Implementation from Scratch**\n",
        "**Course:** Deep Learning  \n",
        "---\n",
        "\n",
        "## **Student Information**\n",
        "**Name :** Tejas Dattatray Gophane\n",
        "\n",
        "**PRN Number :** 202201040063\n",
        "\n",
        "**Batch :** T2  \n",
        "\n",
        "**Date Of Submission :** 28 February 2025\n",
        "\n",
        "---\n",
        "\n",
        "## **Objective**\n",
        "Implement a simple feedforward neural network from scratch in Python without using any in-built deep learning libraries. The network should demonstrate forward propagation, backward propagation (backpropagation), and training using gradient descent.\n",
        "\n",
        "---\n",
        "\n",
        "## **Neural Network Architecture**\n",
        "- Input Layer: 2 neurons (for 2 input features)\n",
        "- Hidden Layer: 4 neurons (with Sigmoid activation)\n",
        "- Output Layer: 1 neuron (Sigmoid activation for binary output)\n",
        "\n",
        "Loss Function: Mean Squared Error (MSE)  \n",
        "Optimizer: Gradient Descent  \n"
      ],
      "metadata": {
        "id": "lMe69xJRzQZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sigmoid and Derivative:**\n",
        "\n",
        "sigmoid(x) computes the output of the sigmoid activation function. It maps any real-valued input into a value between 0 and 1.\n",
        "sigmoid_derivative(x) is the derivative of the sigmoid function, which is used during backpropagation to calculate gradients.\n",
        "**Neural Network Class:**\n",
        "\n",
        "The NeuralNetwork class contains the methods to initialize the network, perform the forward and backward passes, and train the network.\n",
        "**Forward Pass:**\n",
        "\n",
        "The forward pass computes the activations of the neurons in the network.\n",
        "The input is passed through the layers, with the weights and biases applied, followed by the activation function (sigmoid) to compute the output.\n",
        "Backward Pass (Backpropagation):\n",
        "\n",
        "During the backward pass, the weights are updated by calculating the error between the predicted output and the true output.\n",
        "The gradients are calculated using the derivative of the sigmoid function, and the weights and biases are updated using gradient descent with a specified learning rate.\n",
        "Training Method:\n",
        "\n",
        "The network is trained by performing multiple epochs, where each epoch involves a forward pass followed by a backward pass.\n",
        "Every 1000 epochs, the loss (mean squared error) is printed to track the network's progress in learning.\n",
        "Main Program:\n",
        "\n",
        "The main program defines a simple XOR dataset, where the inputs are 0 and 1 combinations, and the output is their XOR result.\n",
        "The network is created with 2 input neurons, 4 hidden neurons, and 1 output neuron.\n",
        "The network is trained on the XOR data for 10,000 epochs with a learning rate of 0.1.\n",
        "Output:\n",
        "\n",
        "After training, the network is tested on the same XOR inputs, and the predictions are printed.\n"
      ],
      "metadata": {
        "id": "Ewv0dano0r92"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3DIc5SozHxa",
        "outputId": "0da13dbb-34b7-4d3c-f26d-289cc12eef2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 - Loss: 0.2988828796025321\n",
            "Epoch 1000 - Loss: 0.23044857854693734\n",
            "Epoch 2000 - Loss: 0.10780622832276593\n",
            "Epoch 3000 - Loss: 0.022540087442443714\n",
            "Epoch 4000 - Loss: 0.010072851448091929\n",
            "Epoch 5000 - Loss: 0.006176550048763158\n",
            "Epoch 6000 - Loss: 0.004372585570354429\n",
            "Epoch 7000 - Loss: 0.003353721053416277\n",
            "Epoch 8000 - Loss: 0.0027059033815540943\n",
            "Epoch 9000 - Loss: 0.002260434336880963\n",
            "\n",
            "Predictions after training:\n",
            "[[0.04160289]\n",
            " [0.95189894]\n",
            " [0.95772405]\n",
            " [0.04375697]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sigmoid Activation Function\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Sigmoid activation function.\n",
        "    It maps any input to a value between 0 and 1.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of Sigmoid Activation Function\n",
        "def sigmoid_derivative(x):\n",
        "    \"\"\"\n",
        "    Derivative of the sigmoid function.\n",
        "    This is used during backpropagation to calculate the gradient.\n",
        "    \"\"\"\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Neural Network Class Definition\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Initialize the neural network with the given sizes for the input, hidden, and output layers.\n",
        "\n",
        "        - input_size: Number of input features\n",
        "        - hidden_size: Number of neurons in the hidden layer\n",
        "        - output_size: Number of output neurons\n",
        "        \"\"\"\n",
        "        # Randomly initialize the weights and biases\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # Weights and biases initialization with random values\n",
        "        self.weights_input_hidden = np.random.randn(self.input_size, self.hidden_size)  # Weights from input to hidden layer\n",
        "        self.bias_hidden = np.random.randn(1, self.hidden_size)  # Bias for hidden layer\n",
        "\n",
        "        self.weights_hidden_output = np.random.randn(self.hidden_size, self.output_size)  # Weights from hidden to output layer\n",
        "        self.bias_output = np.random.randn(1, self.output_size)  # Bias for output layer\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Perform the forward pass of the neural network.\n",
        "        Compute the activations for the input, hidden, and output layers.\n",
        "\n",
        "        - X: Input data (features)\n",
        "\n",
        "        Returns the output of the network.\n",
        "        \"\"\"\n",
        "        self.input_layer = X  # Store the input data\n",
        "\n",
        "        # Calculate the input to the hidden layer and apply the activation function\n",
        "        self.hidden_layer_input = np.dot(self.input_layer, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_layer_output = sigmoid(self.hidden_layer_input)\n",
        "\n",
        "        # Calculate the input to the output layer and apply the activation function\n",
        "        self.output_layer_input = np.dot(self.hidden_layer_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.output_layer_output = sigmoid(self.output_layer_input)\n",
        "\n",
        "        return self.output_layer_output\n",
        "\n",
        "    def backward(self, X, y, learning_rate):\n",
        "        \"\"\"\n",
        "        Perform the backward pass of the neural network (backpropagation).\n",
        "        This step adjusts the weights based on the error in the output.\n",
        "\n",
        "        - X: Input data (features)\n",
        "        - y: True labels (targets)\n",
        "        - learning_rate: The rate at which the weights are adjusted\n",
        "        \"\"\"\n",
        "        # Compute the error in the output layer\n",
        "        error_output = y - self.output_layer_output\n",
        "\n",
        "        # Calculate the gradient (delta) for the output layer\n",
        "        output_layer_delta = error_output * sigmoid_derivative(self.output_layer_output)\n",
        "\n",
        "        # Compute the error in the hidden layer\n",
        "        error_hidden = output_layer_delta.dot(self.weights_hidden_output.T)\n",
        "\n",
        "        # Calculate the gradient (delta) for the hidden layer\n",
        "        hidden_layer_delta = error_hidden * sigmoid_derivative(self.hidden_layer_output)\n",
        "\n",
        "        # Update the weights and biases using the computed gradients\n",
        "        # Update weights from hidden to output layer\n",
        "        self.weights_hidden_output += self.hidden_layer_output.T.dot(output_layer_delta) * learning_rate\n",
        "\n",
        "        # Update bias for the output layer\n",
        "        self.bias_output += np.sum(output_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "        # Update weights from input to hidden layer\n",
        "        self.weights_input_hidden += X.T.dot(hidden_layer_delta) * learning_rate\n",
        "\n",
        "        # Update bias for the hidden layer\n",
        "        self.bias_hidden += np.sum(hidden_layer_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        \"\"\"\n",
        "        Train the neural network on the provided data using the forward and backward passes.\n",
        "\n",
        "        - X: Input data (features)\n",
        "        - y: True labels (targets)\n",
        "        - epochs: Number of times to iterate through the entire dataset\n",
        "        - learning_rate: Rate at which the weights are adjusted\n",
        "        \"\"\"\n",
        "        for epoch in range(epochs):\n",
        "            # Perform a forward pass\n",
        "            self.forward(X)\n",
        "\n",
        "            # Perform a backward pass (backpropagation)\n",
        "            self.backward(X, y, learning_rate)\n",
        "\n",
        "            # Print loss (mean squared error) every 1000 epochs\n",
        "            if epoch % 1000 == 0:\n",
        "                loss = np.mean(np.square(y - self.output_layer_output))  # Mean squared error\n",
        "                print(f\"Epoch {epoch} - Loss: {loss}\")\n",
        "\n",
        "# Main Program\n",
        "if __name__ == \"__main__\":\n",
        "    # Define the XOR problem as a simple example\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input data (XOR inputs)\n",
        "    y = np.array([[0], [1], [1], [0]])  # Expected output data (XOR outputs)\n",
        "\n",
        "    # Create an instance of the NeuralNetwork class with:\n",
        "    # 2 input neurons, 4 hidden neurons, and 1 output neuron\n",
        "    nn = NeuralNetwork(input_size=2, hidden_size=4, output_size=1)\n",
        "\n",
        "    # Train the network with the XOR dataset for 10,000 epochs and a learning rate of 0.1\n",
        "    nn.train(X, y, epochs=10000, learning_rate=0.1)\n",
        "\n",
        "    # After training, print the final predictions of the network\n",
        "    print(\"\\nPredictions after training:\")\n",
        "    print(nn.forward(X))  # Test the network on the XOR inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluate the model performance after training\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Predictions on XOR dataset\n",
        "predictions = nn.forward(X).round()\n",
        "\n",
        "# Accuracy calculation\n",
        "accuracy = accuracy_score(y, predictions)\n",
        "print(\"\\nModel Accuracy:\", accuracy)\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y, predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i7NbhyS0lLR",
        "outputId": "21517b41-e0d9-47ce-cafa-88e7f4264a9c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Accuracy: 1.0\n",
            "Confusion Matrix:\n",
            " [[2 0]\n",
            " [0 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I am Tejas Gophane , confirm that the work submitted in this assignment is my own and has been completed following academic integrity guidelines. The code is uploaded to my GitHub repository.\n",
        "\n",
        "**GitHub Repository Link :**\n",
        "\n",
        "**Signature :** Tejas Dattatray Gophane\n"
      ],
      "metadata": {
        "id": "ptoK6LEV1Bqm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7hAFxWFS0562"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}